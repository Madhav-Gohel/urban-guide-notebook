{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae88b94f",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "from pypdf import PdfReader\n",
    "\n",
    "documents = [\n",
    "  \n",
    "]\n",
    "reader = PdfReader(\"file2.pdf\")\n",
    "number_of_pages = len(reader.pages)\n",
    "for i in range(len(reader.pages)):\n",
    "    page = reader.pages[i]\n",
    "    documents.append(page.extract_text())\n",
    "\n",
    "client = chromadb.Client()\n",
    "try:\n",
    "    collection = client.create_collection(name=\"docs\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# store each document in a vector embedding database\n",
    "for i, d in enumerate(documents):\n",
    "  response = ollama.embeddings(model=\"mxbai-embed-large\", prompt=d)\n",
    "  embedding = response[\"embedding\"]\n",
    "  collection.add(\n",
    "    ids=[str(i)],\n",
    "    embeddings=[embedding],\n",
    "    documents=[d]\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0615492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bef57e73",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "import requests\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "# Define your SerpAPI key\n",
    "SERPAPI_API_KEY = \"\"\n",
    "\n",
    "# Function to search Google using SerpAPI\n",
    "def google_search(query):\n",
    "    search = GoogleSearch({\n",
    "        \"q\": query,\n",
    "        \"api_key\": SERPAPI_API_KEY,\n",
    "        \"google_domain\": \"google.co.in\"\n",
    "    })\n",
    "    results = search.get_dict()\n",
    "    return results['organic_results'] if 'organic_results' in results else []\n",
    "\n",
    "\n",
    "def chatbot(prompt, chat_history):\n",
    "    # Generate an embedding for the prompt\n",
    "    response = ollama.embeddings(\n",
    "        prompt=prompt,\n",
    "        model=\"mxbai-embed-large\"\n",
    "    )\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[response[\"embedding\"]],\n",
    "        n_results=3\n",
    "    )\n",
    "    \n",
    "    # Extract the data from the result\n",
    "    data = results['documents'][0][0]\n",
    "    \n",
    "    # Fetch real-time data from Google Search\n",
    "    google_results = google_search(prompt)\n",
    "    \n",
    "    # Combine data from knowledge base and Google search results\n",
    "    combined_data = f\"Knowledge base data: '{data}'. Google search results: {google_results[:3]}. \"\n",
    "    \n",
    "    # Generate a response using the \"mistral-small\" model with streaming\n",
    "    full_response = \"\"\n",
    "    for chunk in ollama.generate(\n",
    "#         model=\"llama3.1\",\n",
    "        model=\"mistral-small\",\n",
    "        prompt=f\"Using this data: '{combined_data}' and your knowledge, respond to this prompt with explanation of why this answer only and answer should be given first then explaination: {prompt}\",\n",
    "#         prompt=f\"Using this data: '{combined_data}' and your knowledge, respond to this prompt:'{prompt}' and also highlight the answer in bold so it is visible\",\n",
    "        stream=True\n",
    "    ):\n",
    "        full_response += chunk['response']\n",
    "        yield chat_history + [(prompt, full_response)]\n",
    "    \n",
    "    # Append user input and model output to the chat history\n",
    "    chat_history.append((prompt, full_response))\n",
    "    yield chat_history\n",
    "\n",
    "# Create the Gradio chatbot interface\n",
    "with gr.Blocks(title=\"RAG with Google Search\") as chatbot_ui:\n",
    "    gr.Markdown(\"\"\"<h1><center>Urban Guide: A RAG-Based City Assistant Using LLMs and Agents</center></h1>\"\"\")\n",
    "    chatbot_history = gr.Chatbot()  # For displaying the chat-like conversation\n",
    "    user_input = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")  # Textbox for user input\n",
    "    submit_button = gr.Button(\"Send\")  # Submit button\n",
    "    \n",
    "    # Define the interaction flow\n",
    "    submit_button.click(chatbot, inputs=[user_input, chatbot_history], outputs=chatbot_history)\n",
    "    user_input.submit(chatbot, inputs=[user_input, chatbot_history], outputs=chatbot_history)\n",
    "\n",
    "# Launch the interface\n",
    "chatbot_ui.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc980d2f",
   "metadata": {},
   "source": [
    "## Without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "# import requests\n",
    "# from serpapi import GoogleSearch\n",
    "\n",
    "# Define your SerpAPI key\n",
    "# SERPAPI_API_KEY = \"\"\n",
    "\n",
    "# # Function to search Google using SerpAPI\n",
    "# def google_search(query):\n",
    "#     search = GoogleSearch({\n",
    "#         \"q\": query,\n",
    "#         \"api_key\": SERPAPI_API_KEY,\n",
    "#         \"google_domain\": \"google.co.in\"\n",
    "#     })\n",
    "#     results = search.get_dict()\n",
    "#     return results['organic_results'] if 'organic_results' in results else []\n",
    "\n",
    "\n",
    "def chatbot(prompt, chat_history):\n",
    "    # Generate an embedding for the prompt\n",
    "#     response = ollama.embeddings(\n",
    "#         prompt=prompt,\n",
    "#         model=\"mxbai-embed-large\"\n",
    "#     )\n",
    "    \n",
    "#     results = collection.query(\n",
    "#         query_embeddings=[response[\"embedding\"]],\n",
    "#         n_results=1\n",
    "#     )\n",
    "    \n",
    "#     # Extract the data from the result\n",
    "#     data = results['documents'][0][0]\n",
    "    \n",
    "    # Fetch real-time data from Google Search\n",
    "#     google_results = google_search(prompt)\n",
    "    \n",
    "    # Combine data from knowledge base and Google search results\n",
    "#     combined_data = f\"Knowledge base data: '{data}'. Google search results: {google_results[:3]}.\"\n",
    "    \n",
    "    # Generate a response using the \"mistral-small\" model with streaming\n",
    "    full_response = \"\"\n",
    "    for chunk in ollama.generate(\n",
    "#         model=\"llama3.1\",\n",
    "        model=\"qwen2.5-coder:32b\",\n",
    "        prompt=prompt,\n",
    "#         prompt=f\"Using this data: '{combined_data}' and your knowledge, respond to this prompt:'{prompt}' and also highlight the answer in bold so it is visible\",\n",
    "        stream=True\n",
    "    ):\n",
    "        full_response += chunk['response']\n",
    "        yield chat_history + [(prompt, full_response)]\n",
    "    \n",
    "    # Append user input and model output to the chat history\n",
    "    chat_history.append((prompt, full_response))\n",
    "    yield chat_history\n",
    "\n",
    "# Create the Gradio chatbot interface\n",
    "with gr.Blocks(title=\"Qwen Coder\") as chatbot_ui:\n",
    "    gr.Markdown(\"\"\"<h1><center>Coder</center></h1>\"\"\")\n",
    "    chatbot_history = gr.Chatbot()  # For displaying the chat-like conversation\n",
    "    user_input = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")  # Textbox for user input\n",
    "    submit_button = gr.Button(\"Send\")  # Submit button\n",
    "    \n",
    "    # Define the interaction flow\n",
    "    submit_button.click(chatbot, inputs=[user_input, chatbot_history], outputs=chatbot_history)\n",
    "    user_input.submit(chatbot, inputs=[user_input, chatbot_history], outputs=chatbot_history)\n",
    "\n",
    "# Launch the interface\n",
    "chatbot_ui.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafbbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "186a849f",
   "metadata": {},
   "source": [
    "### Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c62b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "\n",
    "# Define the function that will be used in the Gradio interface\n",
    "def chatbot(prompt, chat_history):\n",
    "    # Generate an embedding for the prompt\n",
    "    response = ollama.embeddings(\n",
    "        prompt=prompt,\n",
    "        model=\"mxbai-embed-large\"\n",
    "    )\n",
    "    \n",
    "    # Query the collection to get the most relevant document\n",
    "    results = collection.query(\n",
    "        query_embeddings=[response[\"embedding\"]],\n",
    "        n_results=1\n",
    "    )\n",
    "    \n",
    "    # Extract the data from the result\n",
    "    data = results['documents'][0][0]\n",
    "    \n",
    "    # Generate a response using the \"mistral-small\" model\n",
    "    output = ollama.generate(\n",
    "        model=\"mistral-small\",\n",
    "        prompt=f\"Using this data: '{data}' and your knowledge. Respond to this prompt: {prompt}\"\n",
    "    )\n",
    "    \n",
    "    # Append user input and model output to the chat history\n",
    "    chat_history.append((prompt, output['response']))\n",
    "    return chat_history, chat_history\n",
    "\n",
    "# Create the Gradio chatbot interface\n",
    "with gr.Blocks(title=\"Urban Guide\") as chatbot_ui:\n",
    "    gr.Markdown(\"\"\"<h1><center> Urban Guide (Custom Knowledge base QA with LLMs) </center></h1>\"\"\")\n",
    "    chatbot_history = gr.Chatbot()  # For displaying the chat-like conversation\n",
    "    user_input = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")  # Textbox for user input\n",
    "    submit_button = gr.Button(\"Send\")  # Submit button\n",
    "    \n",
    "    gr.Markdown('<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*diTLYX2NBstoDrSjLglM5g.png\"/></center>')\n",
    "\n",
    "    # Define the interaction flow\n",
    "    submit_button.click(chatbot, inputs=[user_input, chatbot_history], outputs=[chatbot_history, chatbot_history])\n",
    "    user_input.submit(chatbot, inputs=[user_input, chatbot_history], outputs=[chatbot_history, chatbot_history])\n",
    "\n",
    "# Launch the interface\n",
    "chatbot_ui.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30838702",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea79d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return f\"Hello {name}!\"\n",
    "\n",
    "iface = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a74a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='mistral-small', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'give me a simple code for retrival augmented generation using langchain and ollama',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ad91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "import requests\n",
    "import wikipedia\n",
    "from serpapi import GoogleSearch  # Import SerpAPI for Google search\n",
    "\n",
    "# Define your SerpAPI key\n",
    "SERPAPI_API_KEY = \"\"\n",
    "\n",
    "# Function to search Google using SerpAPI\n",
    "def google_search(query):\n",
    "    search = GoogleSearch({\n",
    "        \"q\": query,\n",
    "        \"api_key\": SERPAPI_API_KEY,\n",
    "        \"google_domain\": \"google.co.in\"\n",
    "    })\n",
    "    results = search.get_dict()\n",
    "    return results['organic_results'] if 'organic_results' in results else []\n",
    "\n",
    "# Define the function that will be used in the Gradio interface\n",
    "def chatbot(prompt, chat_history):\n",
    "    # Generate an embedding for the prompt\n",
    "    response = ollama.embeddings(\n",
    "        prompt=prompt,\n",
    "        model=\"mxbai-embed-large\"\n",
    "    )\n",
    "    \n",
    "    # Query the collection to get the most relevant document\n",
    "    results = collection.query(\n",
    "        query_embeddings=[response[\"embedding\"]],\n",
    "        n_results=1\n",
    "    )\n",
    "    \n",
    "    # Extract the data from the result\n",
    "    data = results['documents'][0][0]\n",
    "    \n",
    "    if \"who is\" in prompt:\n",
    "        wikidata = wikipedia.summary(\"Ratan Tata\", 3)\n",
    "    else:\n",
    "        wikidata = \"\"\n",
    "    \n",
    "    # Fetch real-time data from Google Search\n",
    "    google_results = google_search(prompt)\n",
    "    \n",
    "    # Combine data from knowledge base and Google search results\n",
    "    combined_data = f\"Knowledge base data: '{data}'. Google search results: {google_results[:3]}. Wikipedia Data: {wikidata}\"\n",
    "    \n",
    "    # Generate a response using the \"mistral-small\" model\n",
    "    output = ollama.generate(\n",
    "        model=\"mistral-small\",\n",
    "        prompt=f\"Using this data: '{combined_data}' and your knowledge, respond to this prompt with explaination of why this answer only: {prompt}\"\n",
    "    )\n",
    "    \n",
    "    # Append user input and model output to the chat history\n",
    "    chat_history.append((prompt, output['response']))\n",
    "    return chat_history, chat_history\n",
    "\n",
    "# Create the Gradio chatbot interface\n",
    "with gr.Blocks(title=\"RAG with Google Search\") as chatbot_ui:\n",
    "    gr.Markdown(\"\"\"<h1><center>Custom Knowledge Base QA with LLMs and Internet Data</center></h1>\"\"\")\n",
    "    chatbot_history = gr.Chatbot()  # For displaying the chat-like conversation\n",
    "    user_input = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")  # Textbox for user input\n",
    "    submit_button = gr.Button(\"Send\")  # Submit button\n",
    "    \n",
    "#     gr.Markdown('<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*diTLYX2NBstoDrSjLglM5g.png\"/></center>')\n",
    "\n",
    "    # Define the interaction flow\n",
    "    submit_button.click(chatbot, inputs=[user_input, chatbot_history], outputs=[chatbot_history, chatbot_history])\n",
    "    user_input.submit(chatbot, inputs=[user_input, chatbot_history], outputs=[chatbot_history, chatbot_history])\n",
    "\n",
    "# Launch the interface\n",
    "chatbot_ui.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43802505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82329a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "import requests\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "# Define your SerpAPI key\n",
    "SERPAPI_API_KEY = \"your_serpapi_api_key\"\n",
    "\n",
    "# Function to search Google using SerpAPI\n",
    "def google_search(query):\n",
    "    search = GoogleSearch({\n",
    "        \"q\": query,\n",
    "        \"api_key\": SERPAPI_API_KEY\n",
    "    })\n",
    "    results = search.get_dict()\n",
    "    return results['organic_results'] if 'organic_results' in results else []\n",
    "\n",
    "# Function to search Google Maps using SerpAPI\n",
    "def google_maps_search(query, location=\"\"):\n",
    "    search_params = {\n",
    "        \"q\": query,\n",
    "        \"location\": location,  # Optional location parameter\n",
    "        \"api_key\": SERPAPI_API_KEY,\n",
    "        \"engine\": \"google_maps\"  # Use Google Maps search engine\n",
    "    }\n",
    "    search = GoogleSearch(search_params)\n",
    "    results = search.get_dict()\n",
    "    return results['local_results'] if 'local_results' in results else []\n",
    "\n",
    "# Define the function that will be used in the Gradio interface\n",
    "def chatbot(prompt, chat_history):\n",
    "    # Generate an embedding for the prompt\n",
    "    response = ollama.embeddings(\n",
    "        prompt=prompt,\n",
    "        model=\"mxbai-embed-large\"\n",
    "    )\n",
    "    \n",
    "    # Query the collection to get the most relevant document\n",
    "    results = collection.query(\n",
    "        query_embeddings=[response[\"embedding\"]],\n",
    "        n_results=1\n",
    "    )\n",
    "    \n",
    "    # Check if a document is available in the knowledge base\n",
    "    if results['documents']:\n",
    "        # Extract the data from the knowledge base\n",
    "        data = results['documents'][0][0]\n",
    "        source = \"Knowledge base\"\n",
    "    else:\n",
    "        # If no document is found, try Google Maps for location-based queries\n",
    "        google_maps_results = google_maps_search(prompt)\n",
    "        if google_maps_results:\n",
    "            # Extract relevant location from Google Maps results\n",
    "            data = f\"Here is a relevant location: {google_maps_results[0]['title']} at {google_maps_results[0]['address']}\"\n",
    "            source = \"Google Maps\"\n",
    "        else:\n",
    "            # If no location is found, perform a regular Google search\n",
    "            google_results = google_search(prompt)\n",
    "            if google_results:\n",
    "                data = google_results[0]['snippet']  # Extract snippet from Google search\n",
    "                source = \"Google search\"\n",
    "            else:\n",
    "                return chat_history, chat_history.append((prompt, \"Sorry, no data available from both the knowledge base and Google search.\"))\n",
    "    \n",
    "    # Combine data and source\n",
    "    combined_data = f\"Using {source} data: '{data}'.\"\n",
    "\n",
    "    # Generate a response using the \"mistral-small\" model\n",
    "    output = ollama.generate(\n",
    "        model=\"mistral-small\",\n",
    "        prompt=f\"{combined_data} Respond to this prompt: {prompt}\"\n",
    "    )\n",
    "    \n",
    "    # Append user input and model output to the chat history\n",
    "    chat_history.append((prompt, output['response']))\n",
    "    return chat_history, chat_history\n",
    "\n",
    "# Create the Gradio chatbot interface\n",
    "with gr.Blocks(title=\"RAG with Google Search and Maps\") as chatbot_ui:\n",
    "    gr.Markdown(\"\"\"<h1><center>Custom Knowledge Base QA with LLMs, Google Search, and Maps</center></h1>\"\"\")\n",
    "    chatbot_history = gr.Chatbot()  # For displaying the chat-like conversation\n",
    "    user_input = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")  # Textbox for user input\n",
    "    submit_button = gr.Button(\"Send\")  # Submit button\n",
    "    \n",
    "#     gr.Markdown('<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*diTLYX2NBstoDrSjLglM5g.png\"/></center>')\n",
    "\n",
    "    # Define the interaction flow\n",
    "    submit_button.click(chatbot, inputs=[user_input, chatbot_history], outputs=[chatbot_history, chatbot_history])\n",
    "    user_input.submit(chatbot, inputs=[user_input, chatbot_history], outputs=[chatbot_history, chatbot_history])\n",
    "\n",
    "# Launch the interface\n",
    "chatbot_ui.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywhatkit as kit\n",
    "\n",
    "# Fetch a summary from Wikipedia\n",
    "kit.info(\"Python programming language\", web=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "import requests\n",
    "from serpapi import GoogleSearch  # Import SerpAPI for Google search\n",
    "\n",
    "# Define your SerpAPI key\n",
    "SERPAPI_API_KEY = \"apikey\"\n",
    "\n",
    "# Function to search Google using SerpAPI\n",
    "def google_search(query):\n",
    "    search = GoogleSearch({\n",
    "        \"q\": query,\n",
    "        \"api_key\": SERPAPI_API_KEY,\n",
    "        \"google_domain\": \"google.co.in\"\n",
    "    })\n",
    "    results = search.get_dict()\n",
    "    return results['organic_results'] if 'organic_results' in results else []\n",
    "\n",
    "# Define the function that will be used in the Gradio interface\n",
    "def chatbot(prompt, chat_history):\n",
    "    # Generate an embedding for the prompt\n",
    "    response = ollama.embeddings(\n",
    "        prompt=prompt,\n",
    "        model=\"mxbai-embed-large\"\n",
    "    )\n",
    "    \n",
    "    # Query the collection to get the most relevant document\n",
    "    results = collection.query(\n",
    "        query_embeddings=[response[\"embedding\"]],\n",
    "        n_results=1\n",
    "    )\n",
    "    \n",
    "    # Extract the data from the result\n",
    "    data = results['documents'][0][0]\n",
    "    \n",
    "    \n",
    "    # Fetch real-time data from Google Search\n",
    "    google_results = google_search(prompt)\n",
    "    \n",
    "    # Combine data from knowledge base and Google search results\n",
    "    combined_data = f\"Knowledge base data: '{data}'. Google search results: {google_results[:3]}.\"\n",
    "    \n",
    "    # Generate a response using the \"mistral-small\" model\n",
    "    output = ollama.generate(\n",
    "        model=\"mistral-small\",\n",
    "        prompt=f\"Using this data: '{combined_data}' and your knowledge, respond to this prompt with explaination of why this answer only: {prompt}\"\n",
    "    )\n",
    "    \n",
    "    # Append user input and model output to the chat history\n",
    "    chat_history.append((prompt, output['response']))\n",
    "    return chat_history, chat_history\n",
    "\n",
    "# Create the Gradio chatbot interface\n",
    "with gr.Blocks(title=\"RAG with Google Search\") as chatbot_ui:\n",
    "    gr.Markdown(\"\"\"<h1><center>Urban Guide: A RAG-Based City Assistant Using LLMs and Agents</center></h1>\"\"\")\n",
    "    chatbot_history = gr.Chatbot()  # For displaying the chat-like conversation\n",
    "    user_input = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")  # Textbox for user input\n",
    "    submit_button = gr.Button(\"Send\")  # Submit button\n",
    "    \n",
    "#     gr.Markdown('<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*diTLYX2NBstoDrSjLglM5g.png\"/></center>')\n",
    "\n",
    "    # Define the interaction flow\n",
    "    submit_button.click(chatbot, inputs=[user_input, chatbot_history], outputs=[chatbot_history, chatbot_history])\n",
    "    user_input.submit(chatbot, inputs=[user_input, chatbot_history], outputs=[chatbot_history, chatbot_history])\n",
    "\n",
    "# Launch the interface\n",
    "chatbot_ui.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea2887",
   "metadata": {},
   "source": [
    "# Currently Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667bab9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c07c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "import requests\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "# Define your SerpAPI key\n",
    "SERPAPI_API_KEY = \"\"\n",
    "\n",
    "# Function to search Google using SerpAPI\n",
    "def google_search(query):\n",
    "    search = GoogleSearch({\n",
    "        \"q\": query,\n",
    "        \"api_key\": SERPAPI_API_KEY,\n",
    "        \"google_domain\": \"google.co.in\"\n",
    "    })\n",
    "    results = search.get_dict()\n",
    "    return results['organic_results'] if 'organic_results' in results else []\n",
    "\n",
    "\n",
    "def chatbot(prompt, chat_history):\n",
    "    # Generate an embedding for the prompt\n",
    "    response = ollama.embeddings(\n",
    "        prompt=prompt,\n",
    "        model=\"mxbai-embed-large\"\n",
    "    )\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[response[\"embedding\"]],\n",
    "        n_results=1\n",
    "    )\n",
    "    \n",
    "    # Extract the data from the result\n",
    "    data = results['documents'][0][0]\n",
    "    \n",
    "    # Fetch real-time data from Google Search\n",
    "    google_results = google_search(prompt)\n",
    "    \n",
    "    # Combine data from knowledge base and Google search results\n",
    "    combined_data = f\"Knowledge base data: '{data}'. Google search results: {google_results[:3]}.\"\n",
    "    \n",
    "    # Generate a response using the \"mistral-small\" model with streaming\n",
    "    full_response = \"\"\n",
    "    for chunk in ollama.generate(\n",
    "#         model=\"llama3.1\",\n",
    "        model=\"mistral-small\",\n",
    "        prompt=f\"Using this data: '{combined_data}' and your knowledge, respond to this prompt with explanation of why this answer only: {prompt}\",\n",
    "        stream=True\n",
    "    ):\n",
    "        full_response += chunk['response']\n",
    "        yield chat_history + [(prompt, full_response)]\n",
    "    \n",
    "    # Append user input and model output to the chat history\n",
    "    chat_history.append((prompt, full_response))\n",
    "    yield chat_history\n",
    "\n",
    "# Create the Gradio chatbot interface\n",
    "with gr.Blocks(title=\"RAG with Google Search\") as chatbot_ui:\n",
    "    gr.Markdown(\"\"\"<h1><center>Urban Guide: A RAG-Based City Assistant Using LLMs and Agents</center></h1>\"\"\")\n",
    "    chatbot_history = gr.Chatbot()  # For displaying the chat-like conversation\n",
    "    user_input = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")  # Textbox for user input\n",
    "    submit_button = gr.Button(\"Send\")  # Submit button\n",
    "    \n",
    "    # Define the interaction flow\n",
    "    submit_button.click(chatbot, inputs=[user_input, chatbot_history], outputs=chatbot_history)\n",
    "    user_input.submit(chatbot, inputs=[user_input, chatbot_history], outputs=chatbot_history)\n",
    "\n",
    "# Launch the interface\n",
    "chatbot_ui.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4543d45b",
   "metadata": {},
   "source": [
    "# With Context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2982ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "import requests\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "# Define your SerpAPI key\n",
    "SERPAPI_API_KEY = \"\"\n",
    "\n",
    "# Function to search Google using SerpAPI\n",
    "def google_search(query):\n",
    "    search = GoogleSearch({\n",
    "        \"q\": query,\n",
    "        \"api_key\": SERPAPI_API_KEY,\n",
    "        \"google_domain\": \"google.co.in\"\n",
    "    })\n",
    "    results = search.get_dict()\n",
    "    return results['organic_results'] if 'organic_results' in results else []\n",
    "\n",
    "class ContextualChatbot:\n",
    "        \n",
    "    def generate_response(self, prompt: str, chat_history: List[Tuple[str, str]]):\n",
    "        # Build the conversation context into the prompt instead\n",
    "        context_prompt = \"\"\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            context_prompt += f\"User: {user_msg}\\nAssistant: {assistant_msg}\\n\\n\"\n",
    "        \n",
    "        # Get data from Knowledgebase\n",
    "        response = ollama.embeddings(\n",
    "            prompt=prompt,\n",
    "            model=\"mxbai-embed-large\"\n",
    "        )\n",
    "\n",
    "        results = collection.query(\n",
    "            query_embeddings=[response[\"embedding\"]],\n",
    "            n_results=1\n",
    "        )\n",
    "\n",
    "        # Extract the data from the result\n",
    "        data = results['documents'][0][0]\n",
    "        google_results = google_search(prompt)\n",
    "        combined_data = f\"Knowledge base data: '{data}'. Data from internet: {google_results[:3]}.\"\n",
    "        # Add current prompt with context\n",
    "        prompt=f\"Using this data: '{combined_data}' and your knowledge, respond to this prompt with explanation of why this answer only: {prompt}\",\n",
    "        \n",
    "        # Generate response\n",
    "        full_response = \"\"\n",
    "        for chunk in ollama.generate(\n",
    "            model=\"mistral-small\",\n",
    "            prompt=full_prompt,\n",
    "            system=system_prompt,\n",
    "            stream=True,\n",
    "            options={\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.9,\n",
    "            }\n",
    "        ):\n",
    "            full_response += chunk['response']\n",
    "            yield chat_history + [(prompt, full_response)]\n",
    "        \n",
    "        # Update chat history\n",
    "        chat_history.append((prompt, full_response))\n",
    "        yield chat_history\n",
    "\n",
    "# Create chatbot instance\n",
    "chatbot = ContextualChatbot()\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(title=\"RAG with Google Search\") as chatbot_ui:\n",
    "    gr.Markdown(\"\"\"<h1><center>Urban Guide: A RAG-Based City Assistant Using LLMs and Agents</center></h1>\"\"\")\n",
    "    chatbot_history = gr.Chatbot()\n",
    "    user_input = gr.Textbox(\n",
    "        show_label=False, \n",
    "        placeholder=\"Give Your queries here...\",\n",
    "        scale=4\n",
    "    )\n",
    "    submit_button = gr.Button(\"Send\", scale=1)\n",
    "#     clear_button = gr.Button(\"Clear Chat\")\n",
    "    \n",
    "    \n",
    "    # Define the interaction flow\n",
    "    submit_button.click(\n",
    "        chatbot.generate_response, \n",
    "        inputs=[user_input, chatbot_history], \n",
    "        outputs=chatbot_history\n",
    "    )\n",
    "    user_input.submit(\n",
    "        chatbot.generate_response, \n",
    "        inputs=[user_input, chatbot_history], \n",
    "        outputs=chatbot_history\n",
    "    )\n",
    "#     clear_button.click(lambda: None, None, chatbot_history, queue=False)\n",
    "\n",
    "# Launch the interface\n",
    "chatbot_ui.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e757af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "# import requests\n",
    "\n",
    "\n",
    "def chatbot(prompt, chat_history):\n",
    "    \n",
    "    # Generate a response using the \"mistral-small\" model with streaming\n",
    "    full_response = \"\"\n",
    "    for chunk in ollama.generate(\n",
    "#         model=\"llama3.1\",\n",
    "        model=\"mistral-small\",\n",
    "#         context=\n",
    "        prompt=prompt,\n",
    "        stream=True\n",
    "    ):\n",
    "        full_response += chunk['response']\n",
    "        yield chat_history + [(prompt, full_response)]\n",
    "    \n",
    "    # Append user input and model output to the chat history\n",
    "    chat_history.append((prompt, full_response))\n",
    "    yield chat_history\n",
    "\n",
    "# Create the Gradio chatbot interface\n",
    "with gr.Blocks(title=\"Code\") as chatbot_ui:\n",
    "\n",
    "    chatbot_history = gr.Chatbot()  # For displaying the chat-like conversation\n",
    "    user_input = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")  # Textbox for user input\n",
    "    submit_button = gr.Button(\"Send\")  # Submit button\n",
    "    \n",
    "    # Define the interaction flow\n",
    "    submit_button.click(chatbot, inputs=[user_input, chatbot_history], outputs=chatbot_history)\n",
    "    user_input.submit(chatbot, inputs=[user_input, chatbot_history], outputs=chatbot_history)\n",
    "\n",
    "# Launch the interface\n",
    "chatbot_ui.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "import json\n",
    "\n",
    "# Function to load chat history\n",
    "def load_history():\n",
    "    try:\n",
    "        with open('chat_history.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Function to save chat history\n",
    "def save_history(history):\n",
    "    with open('chat_history.json', 'w') as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "def chatbot(prompt, chat_history):\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "    \n",
    "    # Generate a response using the \"mistral-small\" model with streaming\n",
    "    full_response = \"\"\n",
    "    for chunk in ollama.generate(\n",
    "        model=\"mistral-small\",\n",
    "        prompt=prompt,\n",
    "        stream=True\n",
    "    ):\n",
    "        full_response += chunk['response']\n",
    "        new_history = chat_history + [(prompt, full_response)]\n",
    "        save_history(new_history)  # Save after each update\n",
    "        yield new_history\n",
    "    \n",
    "    # Final update\n",
    "    chat_history.append((prompt, full_response))\n",
    "    save_history(chat_history)  # Save final state\n",
    "    return chat_history\n",
    "\n",
    "def clear_history():\n",
    "    save_history([])  # Clear saved history\n",
    "    return []\n",
    "\n",
    "# Create the Gradio chatbot interface\n",
    "with gr.Blocks(title=\"Code\") as chatbot_ui:\n",
    "    # Initialize with saved history\n",
    "    chatbot_history = gr.Chatbot(\n",
    "        value=load_history(),\n",
    "        elem_id=\"chatbot\",\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(\n",
    "            show_label=False, \n",
    "            placeholder=\"Type your message here...\",\n",
    "            scale=4,\n",
    "            container=False\n",
    "        )\n",
    "        submit_button = gr.Button(\"Send\", scale=1)\n",
    "    \n",
    "    clear_button = gr.Button(\"Clear History\")\n",
    "\n",
    "    # Define the interaction flow\n",
    "    submit_button.click(\n",
    "        chatbot,\n",
    "        inputs=[user_input, chatbot_history],\n",
    "        outputs=chatbot_history\n",
    "    )\n",
    "\n",
    "    user_input.submit(\n",
    "        chatbot,\n",
    "        inputs=[user_input, chatbot_history],\n",
    "        outputs=chatbot_history\n",
    "    )\n",
    "    \n",
    "    clear_button.click(\n",
    "        clear_history,\n",
    "        outputs=chatbot_history\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "chatbot_ui.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e8e553",
   "metadata": {},
   "source": [
    "## Without Streaming response without rag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "\n",
    "def chatbot(prompt, chat_history):\n",
    "    full_response = \"\"\n",
    "    response = ollama.generate(\n",
    "        model=\"deepseek-r1:32b\",\n",
    "        prompt=prompt\n",
    "    )\n",
    "    full_response += response['response']\n",
    "\n",
    "    # Append user input and model output to the chat history\n",
    "    chat_history.append((prompt, full_response))\n",
    "    return chat_history\n",
    "\n",
    "# Create the Gradio chatbot interface\n",
    "with gr.Blocks(title=\"Qwen Coder\") as chatbot_ui:\n",
    "    gr.Markdown(\"\"\"<h1><center>Coder</center></h1>\"\"\")\n",
    "    chatbot_history = gr.Chatbot()  # For displaying the chat-like conversation\n",
    "    markdown_output = gr.Markdown()  # For displaying Markdown content\n",
    "    user_input = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")  # Textbox for user input\n",
    "    submit_button = gr.Button(\"Send\")  # Submit button\n",
    "\n",
    "    # Define the interaction flow\n",
    "    def update_ui(prompt, chat_history):\n",
    "        result = chatbot(prompt, chat_history)\n",
    "        markdown_content = result[-1][1]  # Extract the latest response as Markdown\n",
    "        return result, markdown_content\n",
    "\n",
    "    submit_button.click(update_ui, inputs=[user_input, chatbot_history], outputs=[chatbot_history, markdown_output])\n",
    "    user_input.submit(update_ui, inputs=[user_input, chatbot_history], outputs=[chatbot_history, markdown_output])\n",
    "\n",
    "# Launch the interface\n",
    "chatbot_ui.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d17549d",
   "metadata": {},
   "source": [
    "# With Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73e1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
